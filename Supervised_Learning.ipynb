{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\n",
        "- Simple Linear Regression (SLR) is a statistical method that models the relationship between a single independent (predictor) variable and a single dependent (outcome) variable, assuming a linear relationship. Its main purpose is to predict the value of the dependent variable based on the independent variable, or to understand the nature and strength of the relationship between them. Purpose of Simple Linear Regression Prediction: SLR is used to make predictions about the dependent variable (\\(y\\)) for a new value of the independent variable (\\(x\\)). For example, it can be used to predict a student's exam score (\\(y\\)) based on the number of hours they studied (\\(x\\)).Understanding relationships: It helps to understand the relationship between the two variables by determining the slope and intercept of the line that best fits the data.Slope (\\(\\beta _{1}\\)): Indicates how much the dependent variable (\\(y\\)) is expected to change for a one-unit increase in the independent variable (\\(x\\)).Intercept (\\(\\beta _{0}\\)): Represents the predicted value of the dependent variable (\\(y\\)) when the independent variable (\\(x\\)) is zero.Foundation for more complex models: Despite its simplicity, SLR is a foundational concept in statistics and machine learning, serving as a basis for understanding more advanced techniques like multiple linear regression."
      ],
      "metadata": {
        "id": "DeqfJRs391vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "- The key assumptions of simple linear regression are linearity, independence, homoscedasticity, and normality of residuals. These assumptions state that there is a linear relationship between variables, the error terms are uncorrelated with each other, the variance of the error terms is constant, and the errors are normally distributed.                    1. Linearity     A linear relationship must exist between the dependent and independent variables. This means a one-unit change in the independent variable (\\(X\\)) is associated with a constant change in the dependent variable (\\(Y\\)).               2. Independence of errors     The error terms (residuals) must be independent of each other.  This means one observation does not influence another, and there is no systematic pattern in the residuals when ordered, such as in time series data.               3. Homoscedasticity     The variance of the error terms must be constant across all levels of the independent variable.  This is often visualized as a scatterplot of residuals where the spread is consistent across the range of predicted values. A \"cone\" shape where the spread increases or decreases is a violation of this assumption.               4. Normality of residuals     The error terms (residuals) must be normally distributed.  This assumption is about the distribution of the errors, not the raw data variables themselves.  It is typically checked using histograms or Q-Q plots to see if the residuals form a bell-shaped, symmetrical curve."
      ],
      "metadata": {
        "id": "zk9GwFk5-cpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 3: Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "\n",
        "The mathematical equation for a simple linear regression model is \\(y=\\beta _{0}+\\beta _{1}x+\\epsilon \\), where \\(y\\) is the dependent variable, \\(x\\) is the independent variable, \\(\\beta _{0}\\) is the y-intercept, \\(\\beta _{1}\\) is the slope, and \\(\\epsilon \\) is the error term. The estimated regression line is often written as \\(\\^{y}=b_{0}+b_{1}x\\), where \\(\\^{y}\\) is the predicted value of \\(y\\), and \\(b_{0}\\) and \\(b_{1}\\) are the estimated values for the intercept and slope, respectively. Equation: Population model: \\(y=\\beta _{0}+\\beta _{1}x+\\epsilon \\)Sample or estimated model: \\(\\^{y}=b_{0}+b_{1}x\\) Terms explained: \\(y\\) (or \\(\\^{y}\\)): The dependent variable (or the predicted value of the dependent variable). This is the variable you are trying to predict or explain.\\(x\\): The independent variable. This is the variable you are using to make predictions about \\(y\\).\\(\\beta _{0}\\) (or \\(b_{0}\\)): The y-intercept. This is the predicted value of \\(y\\) when \\(x\\) is equal to 0.\\(\\beta _{1}\\) (or \\(b_{1}\\)): The slope or regression coefficient. It represents the average change in the dependent variable (\\(y\\)) for a one-unit increase in the independent variable (\\(x\\)).\\(\\epsilon \\): The error term. It represents the difference between the actual value of \\(y\\) and the predicted value of \\(y\\) (\\(\\^{y}\\)), which accounts for variability not explained by the model. It includes all other factors that influence \\(y\\) besides \\(x\\)."
      ],
      "metadata": {
        "id": "X36jualG_TuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Provide a real-world example where simple linear regression can be\n",
        "applied.\n",
        "\n",
        " - A real-world example where simple linear regression can be applied is to predict a house's price based on its square footage.\n",
        "Independent Variable (Predictor): The size of the house in square feet.\n",
        "Dependent Variable (Outcome): The selling price of the house.\n",
        "Application: A real estate agent or a potential buyer can use historical sales data to build a model. This model would estimate how much the price tends to increase for each additional square foot of space. This allows them to predict a reasonable price for a new property based purely on its size, assuming that size is the primary determinant of price in a given market."
      ],
      "metadata": {
        "id": "M8xI3SRN_a-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is the method of\n",
        " least squares in linear regression?\n",
        "\n",
        "- The method of least squares in linear regression is a statistical technique used to find the \"best-fit\" line through a set of data points. It works by minimizing the sum of the squares of the vertical distances between each data point and the line, known as residuals. This process results in a linear equation that best represents the relationship between two variables, allowing for predictions to be made. How it works Visualizing the data: First, a scatter plot of the data points is created, with the independent variable on the x-axis and the dependent variable on the y-axis.Drawing a line: The goal is to find a straight line that comes as close as possible to all the points.Calculating errors: For each point, the difference between the actual y-value and the y-value predicted by the line (called the residual) is calculated.Squaring the errors: Each of these residuals is squared to make all values positive.Minimizing the sum of squares: The method finds the line where the sum of these squared residuals is as small as possible. This is the \"best-fit\" line.Finding the equation: The method provides the mathematical formulas to calculate the slope (\\(m\\)) and the y-intercept (\\(b\\)) of this line, which forms the equation \\(y=mx+b\\)."
      ],
      "metadata": {
        "id": "PIKdRpW6_x_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is Logistic Regression? How does it differ from Linear Regression.\n",
        "\n",
        "- Logistic regression predicts a categorical outcome, like a yes/no answer, by modeling the probability of an event using an S-shaped curve, while linear regression predicts a continuous outcome, like a house price or temperature, using a straight line to show a linear relationship between variables. The main differences lie in their application (classification vs. regression), output (probability vs. continuous value), and the underlying mathematical function used.  \n",
        "Logistic Regression\n",
        "Purpose: Primarily used for classification problems.\n",
        "Output: Predicts the probability of a categorical outcome (e.g., 0 or 1, yes or no).\n",
        "Model: Uses a logistic or sigmoid function, which results in an S-shaped curve and constrains the output to a range between 0 and 1.\n",
        "Example: Predicting whether an email is spam or not spam, or whether a patient has a disease or not.\n",
        "Linear Regression\n",
        "Purpose: Primarily used for regression problems.\n",
        "Output: Predicts a continuous numerical value (e.g., any number from negative infinity to positive infinity).\n",
        "Model: Uses a linear equation, which results in a straight line.\n",
        "Example: Predicting the price of a house based on its features or the temperature on a given day."
      ],
      "metadata": {
        "id": "N0Xp-2okBCCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Name and briefly describe three common evaluation metrics for regression\n",
        "models.\n",
        "\n",
        "- The method of least squares in linear regression is a statistical technique used to find the \"best-fit\" line through a set of data points. It works by minimizing the sum of the squares of the vertical distances between each data point and the line, known as residuals. This process results in a linear equation that best represents the relationship between two variables, allowing for predictions to be made. How it works Visualizing the data: First, a scatter plot of the data points is created, with the independent variable on the x-axis and the dependent variable on the y-axis.Drawing a line: The goal is to find a straight line that comes as close as possible to all the points.Calculating errors: For each point, the difference between the actual y-value and the y-value predicted by the line (called the residual) is calculated.Squaring the errors: Each of these residuals is squared to make all values positive.Minimizing the sum of squares: The method finds the line where the sum of these squared residuals is as small as possible. This is the \"best-fit\" line.Finding the equation: The method provides the mathematical formulas to calculate the slope (\\(m\\)) and the y-intercept (\\(b\\)) of this line, which forms the equation \\(y=mx+b\\)."
      ],
      "metadata": {
        "id": "eec9oy5uARXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the purpose of the\n",
        " R-squared metric in regression analysis?\n",
        "\n",
        "- R-squared, or the coefficient of determination, indicates the proportion of the variance in a dependent variable that is predictable from the independent variables in a regression model. Its purpose is to measure how well the model fits the data, with a higher R-squared value signifying a better fit, meaning the model can explain a larger percentage of the variability in the outcome. What R-squared tells you: Goodness-of-fit: R-squared shows how closely the data points cluster around the regression line.Predictive power: It quantifies the degree to which the model's predictors explain the variation in the response variable.Strength of relationship: It can be seen as a measure of the strength of the relationship between the model and the dependent variable, typically on a scale from \\(0\\%\\) to \\(100\\%\\).Unexplained variation: It also reveals the percentage of variance that the model doesn't explain, which is due to other factors not included in the model. Example: If a model has an R-squared of \\(0.75\\), it means that \\(75\\%\\) of the variance in the dependent variable can be explained by the independent variables included in the model. The remaining \\(25\\%\\) of the variance is due to other factors."
      ],
      "metadata": {
        "id": "JnDzBni1BZqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 9: Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept. (Include your Python code and output in the code box below.)\n",
        "\n",
        "To fit a simple linear regression model using scikit-learn and print the slope and intercept, follow the Python code below:\n",
        "\n",
        "'''import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "# X should be a 2D array for scikit-learn\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 5, 4, 5, 7, 8, 9, 10, 12])\n",
        "\n",
        "# Create a Linear Regression model object\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the slope (coefficient) and intercept\n",
        "print(f\"Slope (Coefficient): {model.coef_[0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\") '''\n",
        "\n",
        "Output :\n",
        "\n",
        "Slope (Coefficient): 0.98\n",
        "Intercept: 1.93"
      ],
      "metadata": {
        "id": "_DkVor9HB9i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "-  In a simple linear regression model, the slope coefficient represents the average change in the dependent variable for a one-unit increase in the independent variable. The intercept coefficient is the predicted value of the dependent variable when the independent variable is zero. The sign of the slope coefficient indicates the direction of the relationship: a positive coefficient means both variables increase together, while a negative coefficient means one increases as the other decreases. Interpreting the slope coefficient (\\(\\beta _{1}\\)) Magnitude: The numerical value tells you how much the dependent variable is predicted to change for each one-unit increase in the independent variable.Example: If the equation is \\(Y=10+2X\\), the slope coefficient is \\(2\\). For every one-unit increase in \\(X\\), the model predicts that \\(Y\\) will increase by \\(2\\).Direction: The sign shows the direction of the relationship between the two variables.Positive coefficient (\\(\\beta _{1}>0\\)): As the independent variable (\\(X\\)) increases, the dependent variable (\\(Y\\)) also tends to increase.Negative coefficient (\\(\\beta _{1}<0\\)): As the independent variable (\\(X\\)) increases, the dependent variable (\\(Y\\)) tends to decrease. Interpreting the intercept coefficient (\\(\\beta _{0}\\)) Definition: The intercept is the estimated value of the dependent variable (\\(Y\\)) when the independent variable (\\(X\\)) is equal to zero.Example: In the equation \\(Y=10+2X\\), the intercept is \\(10\\). This means that when \\(X\\) is \\(0\\), the predicted value for \\(Y\\) is \\(10\\).Caution: The intercept only has a meaningful interpretation if a value of \\(0\\) for the independent variable is logical within the context of the data. For example, if you are modeling house prices based on square footage, a house with \\(0\\) square footage is not realistic, so the intercept may not have a practical meaning beyond anchoring the line. Putting it together The model's equation, often written as \\(Y=\\beta _{0}+\\beta _{1}X+\\epsilon \\), represents the relationship between the variables. Dependent Variable (\\(Y\\)): The variable you are trying to predict.Independent Variable (\\(X\\)): The variable used to make the prediction.Intercept (\\(\\beta _{0}\\)): The baseline value of \\(Y\\) when \\(X\\) is zero.Slope Coefficient (\\(\\beta _{1}\\)): The predicted change in \\(Y\\) for a one-unit increase in \\(X\\).Error Term (\\(\\epsilon \\)): Represents the part of \\(Y\\) that the model cannot predict."
      ],
      "metadata": {
        "id": "vaq-45gECa71"
      }
    }
  ]
}